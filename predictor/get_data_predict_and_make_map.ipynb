{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for preprocessing the data that is pulled from the FMI API\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rename_cols(df):\n",
    "    df = df.rename(columns={\"Precipitation amount\": \"rainfall_mm\", \"Snow depth\": \"snow_depth_cm\", \"Air temperature\": \"air_temp\", \n",
    "     \"Maximum temperature\": \"max_temp\", \"Minimum temperature\": \"min_temp\", \"Ground minimum temperature\": \"min_ground_temp\"})\n",
    "    cols = ['city','rainfall_mm', 'snow_depth_cm', 'air_temp', 'max_temp', 'min_temp', 'min_ground_temp']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_and_replace(df, dfh):\n",
    "    # This functions makes a union of the daily and hourly observations and replaces some values\n",
    "    merged = pd.concat([df, dfh])\n",
    "    merged2 = merged.mask(merged == \"-\", np.nan)\n",
    "    merged2[[\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]] = merged2[[\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]].astype('float64')\n",
    "    merged2 = merged2[[\"city\",\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]].interpolate(axis=0)\n",
    "    merge3 = merged2\n",
    "    merge3[\"rainfall_mm\"] = merge3[\"rainfall_mm\"].where(merge3[\"rainfall_mm\"] >= 0, 0)\n",
    "    merge3[\"snow_depth_cm\"] = merge3[\"snow_depth_cm\"].mask(merge3[\"snow_depth_cm\"] == -1, 0)\n",
    "    return merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for pulling weather observations data from FMI API, takes Finnish cities as parameters\n",
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "from fmiopendata.utils import read_url\n",
    "from lxml import etree\n",
    "\n",
    "def get_places():\n",
    "    xml = read_url(\"http://opendata.fmi.fi/wfs?service=WFS&version=2.0.0&request=getFeature&storedquery_id=fmi::ef::stations\")\n",
    "    root = etree.fromstring(xml)\n",
    "    locations = dict()\n",
    "    for name in root.findall('.//{http://www.opengis.net/gml/3.2}name'):\n",
    "        if name.attrib['codeSpace'] == 'http://xml.fmi.fi/namespace/locationcode/name':\n",
    "            city = name.text.split(' ', 1)[0]\n",
    "            id = name.getparent().find('.//{http://www.opengis.net/gml/3.2}identifier').text\n",
    "            try:\n",
    "                place = name.text.split(' ', 1)[1]\n",
    "            except:\n",
    "                place = ''\n",
    "            if city in locations:\n",
    "                locations[city].append((place,id))\n",
    "            else:\n",
    "                locations[city] = [(place, id)]\n",
    "\n",
    "    return locations\n",
    "\n",
    "\n",
    "def get_daily_obs(cities, places):\n",
    "\n",
    "    # Retrieve the last 10 days daily observations + todays latest 10h observation\n",
    "    end_time = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "    start_time = end_time - dt.timedelta(days=10)\n",
    "    # Convert times to properly formatted strings\n",
    "    start_time = start_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    end_time = end_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    df = pd.DataFrame({'city': [],'Precipitation amount' : [], 'Air temperature' : [], 'Snow depth' : [], 'Minimum temperature' : [], 'Maximum temperature' : [], 'Ground minimum temperature' : [],})\n",
    "\n",
    "    for c in cities:\n",
    "        plcs = places[c]\n",
    "        appended_data = []\n",
    "        for p in plcs:\n",
    "        # For last 10d we get daily values\n",
    "            obs = download_stored_query(\"fmi::observations::weather::daily::multipointcoverage\",\n",
    "                                args=[\"fmisid=\" + p[1],\n",
    "                                    \"starttime=\" + start_time,\n",
    "                                    \"endtime=\" + end_time])\n",
    "            df2 = pd.DataFrame.from_dict({(i): obs.data[i][j]\n",
    "                                for i in obs.data.keys() \n",
    "                                for j in obs.data[i].keys()},\n",
    "                            orient='index')\n",
    "            df2 = df2.applymap(lambda x: x.get('value'))\n",
    "            if df2.empty == False and (df2['Air temperature'].isnull().all() == False or df2['Ground minimum temperature'].isnull().all() == False):\n",
    "                appended_data.append(df2)\n",
    "        df2 = pd.concat(appended_data)\n",
    "        df2['city'] = c\n",
    "        df2.index = df2.index.date\n",
    "        #df2 = df2.groupby([df2.index]).mean()\n",
    "        df = pd.concat([df,df2])\n",
    "    \n",
    "    df = df.groupby([df.index, \"city\"]).mean().reset_index(level=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_hourly_obs(cities, places, cols):\n",
    "    # Retrieve the last 10 days daily observations + todays latest 10h observation\n",
    "    end_time = dt.datetime.utcnow()\n",
    "    start_time = end_time - dt.timedelta(hours=10)\n",
    "    # Convert times to properly formatted strings\n",
    "    start_time = start_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    end_time = end_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    dfh = pd.DataFrame({'city': [], 'Air temperature' : [], 'Precipitation amount' : [],'Snow depth' : []})\n",
    "\n",
    "    for c in cities:\n",
    "        plcs = places[c]\n",
    "        appended_data = []\n",
    "        for p in plcs:\n",
    "        # For last 10h we get all observations and use these to calculate the \"daily\" observations for today\n",
    "            obs = download_stored_query(\"fmi::observations::weather::multipointcoverage\",\n",
    "                                args=[\"fmisid=\" + p[1],\n",
    "                                    \"starttime=\" + start_time,\n",
    "                                    \"endtime=\" + end_time])\n",
    "            dfh2 = pd.DataFrame.from_dict({(i): obs.data[i][j]\n",
    "                                for i in obs.data.keys() \n",
    "                                for j in obs.data[i].keys()},\n",
    "                            orient='index')\n",
    "            dfh2 = dfh2.applymap(lambda x: x.get('value'))\n",
    "            # We have to calculate some new columns for these hourly observations\n",
    "            if dfh2.empty == False:\n",
    "                dfh2['Minimum temperature'] = dfh2['Air temperature'].min()\n",
    "                dfh2['Maximum temperature'] = dfh2['Air temperature'].max()\n",
    "                dfh2['Air temperature'] = dfh2['Air temperature'].mean()\n",
    "                dfh2['Ground minimum temperature'] = dfh2['Minimum temperature']\n",
    "                dfh2['Precipitation amount'] = dfh2['Precipitation amount'].sum()\n",
    "            # Let's take only last calculated value as daily value for station\n",
    "            dfh2 = dfh2.sort_index(axis=0, ascending=False).head(1)\n",
    "            if dfh2.empty == False and dfh2['Air temperature'].isnull().all() == False:\n",
    "                appended_data.append(dfh2)\n",
    "        dfh2 = pd.concat(appended_data)\n",
    "        dfh2['city'] = c\n",
    "        dfh2.index = dfh2.index.date\n",
    "        dfh2 = dfh2[cols]\n",
    "        dfh = pd.concat([dfh,dfh2])\n",
    "    \n",
    "    # Let's take average of station for that city\n",
    "    dfh = dfh.groupby([dfh.index, \"city\"]).mean().reset_index(level=1)\n",
    "    \n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting features from the weather observations data\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def get_features(df, cities):\n",
    "\n",
    "    appended_data = []\n",
    "    for c in cities:\n",
    "        dff = df.loc[df['city'] == c]\n",
    "        dff[\"rain_sum_7d\"] = dff[\"rainfall_mm\"].replace(-1, 0).shift(1).rolling(7).sum().round(2)\n",
    "        dff[\"snow_depth_cm\"] = dff[\"snow_depth_cm\"].replace(0, np.nan).ffill()\n",
    "        dff[\"snow_var_7d\"] = dff[\"snow_depth_cm\"].replace(-1, 0).shift(1).rolling(7).var().round(2)\n",
    "        dff[\"min_temp_2d\"] = dff[\"min_temp\"].shift(1).rolling(2).min().round(2)\n",
    "        dff[\"max_temp_2d\"] = dff[\"max_temp\"].shift(1).rolling(2).max().round(2)\n",
    "        dff[\"is_neg\"] = dff[\"min_temp\"]<0\n",
    "        dff[\"is_neg\"] = dff[\"is_neg\"].astype(int).shift(1)\n",
    "        dff[\"neg_rate_7d\"] = dff[\"is_neg\"].shift(1).rolling(7).mean().round(2)\n",
    "        dff = dff.bfill()\n",
    "        features = dff.drop(columns=['rainfall_mm', 'snow_depth_cm', 'air_temp', 'max_temp', 'min_temp', 'min_ground_temp', 'is_neg']).sort_index(axis=0, ascending=False)\n",
    "        features = features.head(1).fillna(0)\n",
    "        appended_data.append(features)\n",
    "    \n",
    "    all_features = pd.concat(appended_data)\n",
    "\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part executes all the data loading from APIs (get_obs_from_api.py) and does the preprocessing for data (weather_data_trimming.py).\n",
    "# It takes in a list of cities and starts by searching all the observation stations in theses cities. \n",
    "# It then downloads the observations from these stations and basically takes an average over all the stations in one city.\n",
    "# OBS! When excuting it prints quite a lot of \"No observations found\" because some of these stations are inactive. Don't worry about that.\n",
    "\n",
    "cities = ['Helsinki', 'Kuopio', 'Jyv채skyl채', 'Lahti', 'Oulu']\n",
    "\n",
    "places = get_places()\n",
    "df = get_daily_obs(cities, places)\n",
    "dfh = get_hourly_obs(cities, places, df.columns)\n",
    "\n",
    "df2 = rename_cols(df)\n",
    "dfh2 = rename_cols(dfh)\n",
    "df2 = merge_and_replace(df2, dfh2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part extracts features from the observation data. It returns a numpy array.\n",
    "\n",
    "df3 = get_features(df2, cities)\n",
    "X_pred = df3.to_numpy()\n",
    "date = df3.head(1).index.item() + dt.timedelta(days=1)\n",
    "\n",
    "print(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part downloads the trained model and does the prediction for each city. \n",
    "# The output is a numpy array with Date, City, Binary prediction, Prediction as probability\n",
    "import joblib\n",
    "\n",
    "model = joblib.load('../model/random_forest_model')\n",
    "\n",
    "predictions = []\n",
    "for i, city in enumerate(cities):\n",
    "    single_pred = []\n",
    "    single_pred.append(date.strftime(\"%m/%d/%Y\"))\n",
    "    single_pred.append(city)\n",
    "    y_pred = model.predict(X_pred[i,1:].reshape(1,5))\n",
    "    y_prob = model.predict_proba(X_pred[i,1:].reshape(1,5))\n",
    "    single_pred.append(y_pred[0])\n",
    "    single_pred.append(y_prob[0][1])\n",
    "    predictions.append(single_pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Reading shapefile of map of Finland and creating df for coordinates of cities.\n",
    "shapefile = '../finland_shapefile/fi_1km.shp'\n",
    "\n",
    "finMap = gpd.read_file(shapefile)\n",
    "data = {'City': ['Helsinki', 'Kuopio', 'Jyv채skyl채', 'Lahti', 'Oulu'],\n",
    "           'Latitude': [60.17332, 62.897968, 62.242603, 60.980381, 65.012093],\n",
    "           'Longitude': [24.94102, 27.678171, 25.747257, 25.654988, 25.465076]}\n",
    "\n",
    "longLat = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Creating map of Finland\n",
    "crs = {'init':'EPSG:4326'}\n",
    "geometry = [Point(xy) for xy in zip(longLat['Longitude'], longLat['Latitude'])]\n",
    "geo_df = gpd.GeoDataFrame(longLat, \n",
    "                          crs = crs, \n",
    "                          geometry = geometry)\n",
    "\n",
    "\n",
    "pred = pd.DataFrame(predictions)\n",
    "# print(pred[2])\n",
    "\n",
    "# Putting slip warning to the cities not covered yet\n",
    "geo_df['value'] = pred[2].apply(pd.to_numeric).astype(int)\n",
    "\n",
    "# Putting predicted value to Helsinki.\n",
    "# geo_df.loc[0,'value'] = y_pred\n",
    "print(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating color column\n",
    "geo_df['color'] = geo_df['value'].mask(geo_df['value'] == 1, 'orange')\n",
    "geo_df['color'] = geo_df['color'].mask(geo_df['color'] == 0, 'green')\n",
    "print(geo_df['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create map, green if no warning, orange if warning\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "finMap.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df.plot(ax=ax,c=geo_df['color'], marker='^', markersize=450)\n",
    "ax.annotate('Helsinki', xy=(24.94102, 60.17332), xytext=(3, 3), textcoords=\"offset points\")\n",
    "for x, y, label in zip(geo_df[\"Longitude\"], geo_df[\"Latitude\"], geo_df[\"City\"]):\n",
    "    ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "ax.set_title('Slip warnings in Finland: ' + dt.datetime.now().strftime(\"%d %B %G\"))\n",
    "\n",
    "figure_path = '../figures/' + dt.datetime.now().strftime(\"%Y-%m-%d_%H:%M\") + '.png'\n",
    "plt.savefig(figure_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copyfile(figure_path, '../figures/today.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc8430a40fbc827d38a9eb7a33e6235b87f482008d42fc8d130ebcfb90a224a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
